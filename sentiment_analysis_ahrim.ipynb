{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set (Training and Test set)\n",
    "\n",
    "We used IMDb Dataset which can be downloaded from [here](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "This data set contains 50,000 reviews which is evenly split into two groups: 25,000 reviews for each of training and testing. The reviews for training and testing data sets contain a disjoint set of movies. Therefore, we can assume that the validation result with testing data set can be applicable for other movie reviews.\n",
    "\n",
    "Each group has the same number of positive and negative reviews: a positive review has a score from 7 to 10 while a negative review has a score from 1 to 4. The reviews having score 5 or 6 are excluded to avoid vagueness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "\n",
    "For this project, we used my own Linux machine having AMD Ryzen 7 2700X, 16GB Memory, Geforce RTX 2070.\n",
    "In addition, Keras with Tensorflow backend is used for making a deep learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import os \n",
    "#from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "#\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend.tensorflow_backend as K\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import word_tokenize\n",
    "import glob\n",
    "#from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import sys\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import pandas as pd\n",
    "from keras_tqdm import TQDMNotebookCallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow initial setup\n",
    "\n",
    "To allow Tensorflow to use enough GPU memory, *allow_growth* option is turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data set files\n",
    "\n",
    "First of all, all the documents are loaded. The data sets for training and testing are stored in *data/train* and *data/test*, respectively. For each data set, positive and negative reviews are stored in *pos* and *neg* sub-directories.\n",
    "\n",
    "I have attached the progress bars using the [tqdm](https://github.com/tqdm/tqdm), which is useful in dealting with large data by allowing us to estimate each time of the stages.\n",
    "\n",
    "Referenced article for tqdm: https://towardsdatascience.com/progress-bars-in-python-4b44e8a4c482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training-positive-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fa97261db041c9aa06399243a55c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading training-negative-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd02353291c4c819677099ea25b9bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test-positive-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174dbea0e1b2440bbfbf6436cab010ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test-negative-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d9d5243a2e4d8291371562a8e1df19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load all docs in a directory\n",
    "def load_docs(directory):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        with open(path, 'r') as f:\n",
    "            # load the doc\n",
    "            doc = f.read()\n",
    "            # add to list\n",
    "            documents.append(doc)\n",
    "    return documents\n",
    "\n",
    "# load all training reviews\n",
    "print(\"Loading training-positive-docs\")\n",
    "global_train_positive_docs = load_docs('data/train/pos')\n",
    "print(\"Loading training-negative-docs\")\n",
    "global_train_negative_docs = load_docs('data/train/neg')\n",
    "# load all test reviews\n",
    "print(\"Loading test-positive-docs\")\n",
    "global_test_positive_docs = load_docs('data/test/pos')\n",
    "print(\"Loading test-negative-docs\")\n",
    "global_test_negative_docs = load_docs('data/test/neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning documents\n",
    "\n",
    "### Pre-processing techniques\n",
    "\n",
    "In most of NLP releated works, documents are normally pre-processed to get better performance.\n",
    "We tried to apply several techniques which are well-known as follows:\n",
    "\n",
    "**1. Removing punctuations**  \n",
    "Normally punctuations do not have any meaning, but they exist for understandability. Therefore, such punctuations should be removed. But, we did not remove the apostrophe mark (') since such removing caused the incorrect stemming.\n",
    "\n",
    "**2. Removing stopwords**  \n",
    "We filtered out the stopwords.\n",
    "The stop words are those words that do not contribute to the deeper meaning of the phrase.\n",
    "They are the most common words such as: “the“, “a“, and “is“.\n",
    "NLTK provides a list of commonly agreed upon stop words for a variety of languages.\n",
    "\n",
    "**3. Stemming**    \n",
    "The *PorterStemmer* is provided in *NLTK python package*.\n",
    "We made the words into lowercases and used the stemming method in order to both reduce the vocabulary and to focus on the sense or sentiment of a document rather than deeper meaning.\n",
    "\n",
    "**4. Removing non-frequent words**   \n",
    "It is important to define a vocabulary of known words when using a bag-of-words or embedding model.\n",
    "The more words, the larger the representation of documents, therefore it is important to constrain the words to only those believed to be predictive. \n",
    "\n",
    "In this project, **we set up the vocabulary dictionary by removing the non-frequent words to prevent a model from overfitting.** \n",
    "This is implemeted in [*vocab.ipynb*](https://github.com/ahrimhan/data-science-project/tree/master/project2/src/vocab.ipynb).\n",
    "\n",
    "* After removing all words that have a length <= 1 character, we first construct the vocabulary dictionary based on only reviews in the training dataset (Number of vocabularies: 52,826).\n",
    "* Then, we iterate the vocabulary dictionary again for counting the word occurrences and removing the non-frequent words that have a low occurrence, such as only being used once or none. Thus, remaining vocabularies have the two or more occurrences (Number of filtered vocabularies: 30,819). These filtered vocabularies are saved in [*vocab.txt*](https://github.com/ahrimhan/data-science-project/tree/master/project2/src/vocab.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation_table = str.maketrans('', '', '\\'\"!.,?:;')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# turn a doc into clean tokens\n",
    "vocab = []\n",
    "\n",
    "with open('./vocab/vocab.txt') as f:\n",
    "    vocab = f.read().split() \n",
    "    \n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # remove punctuation from each token\n",
    "    tokens = [w.translate(remove_punctuation_table) for w in tokens]\n",
    "    \n",
    "    # remove stop words\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    \n",
    "    # stemming\n",
    "    porter = PorterStemmer()\n",
    "    tokens = [porter.stem(w.lower()) for w in tokens]\n",
    "\n",
    "    # filter out tokens not in vocab\n",
    "    if len(vocab) > 0:\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "    \n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing\n",
    "\n",
    "Pre-processing mentioned above requires heavy computation. \n",
    "To improve the speed, we parallelized the pre-processing using the *Pool module* in *multiprocessing package*.\n",
    "Since we use a CPU having 8 cores, the size of Pool is set as 8.\n",
    "By using this technique, **we could achieve 6~7 times speed up.** \n",
    "Using the single thread, it takes 10~12 minutes for cleaning up 12500 documents, whereas, using the multiple threads, it takes only 1 minute and 20~40 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up for training-positive-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dfa17badeb44b9a445d1cf16d61a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up for training-negative-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a2422d041e4a75a4b06dd64fdc6bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up for test-positive-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c7b178fb634fcf95632886532554ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up for test-negative-docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcaf363b3f7840788ef546d44a7d00b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Serial version of clean_docs function\n",
    "# def clean_docs(documents):\n",
    "#     for doc in tqdm(documents):\n",
    "#         clean_doc(doc)\n",
    "\n",
    "# Parallel version of clean_docs function\n",
    "def clean_docs(documents):\n",
    "    # Since we use a CPU having 8 cores, the size of Pool is set as 8\n",
    "    with Pool(8) as p:\n",
    "        return list(tqdm(p.imap(clean_doc, documents), total=len(documents)))\n",
    "\n",
    "print(\"Cleaning up for training-positive-docs\")\n",
    "cleaned_train_positive_docs = clean_docs(global_train_positive_docs)\n",
    "print(\"Cleaning up for training-negative-docs\")\n",
    "cleaned_train_negative_docs = clean_docs(global_train_negative_docs)\n",
    "print(\"Cleaning up for test-positive-docs\")\n",
    "cleaned_test_positive_docs = clean_docs(global_test_positive_docs)\n",
    "print(\"Cleaning up for test-negative-docs\")\n",
    "cleaned_test_negative_docs = clean_docs(global_test_negative_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nt', 'realli', 'consid', 'conserv', 'nt', 'person', 'offend', 'film', 'pretti', 'clear', 'plot', 'character', 'film', 'secondari', 'messag', 'and', 'messag', 'conserv', 'either', 'evil', 'stupid', 'charact', 'either', 'good', 'american', 'brainless', 'greedi', 'evil', 'conserv', 'there', 'noth', 'clever', 'creativ', 'nt', 'realli', 'mind', 'polit', 'bia', 'nt', 'purpos', 'behind', 'movi', 'and', 'clearli', 'br', 'br', 'on', 'posit', 'side', 'cast', 'wonder', 'chri', 'cooper', 'impress', 'funni', 'first', 'two', 'three', 'time', 'old', 'joke', 'told', 'br', 'br', 'so', 'realli', 'hate', 'conserv', 'probabl', 'enjoy', 'film', 'look', 'someth', 'realist', 'charact', 'stori', 'less', 'better', 'watch', 'someth', 'els']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_test_negative_docs[0].split()[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding data set into sequence\n",
    "\n",
    "To use documents as an input of a model, each document is encoded as a sequence object of Keras.\n",
    "The function below encodes the documents as sequence objects as well as creates a list of labels: '0' for negative reviews and '1' for positive reviews.\n",
    "We do not need the one hot encoding process (a function called *to_categorical()* in Keras) because there is only two classes of positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data_set(tokenizer, positive_docs, negative_docs, max_word_length):\n",
    "    docs = negative_docs + positive_docs\n",
    "    # sequence encode\n",
    "    encoded_docs = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    x = pad_sequences(encoded_docs, maxlen=max_word_length, padding='post')\n",
    "    # define training labels\n",
    "    y = array(([0] * len(negative_docs)) + ([1] * len(positive_docs)))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Word embedding is a common technique to deal with texts in Deep Learning.\n",
    "To compare the effectiveness of use of pre-trained word embedding, here, both of pre-trained word embedding and new (not-trained) embedding will be used.\n",
    "\n",
    "## Pre-trained word embedding\n",
    "In this project, we will be using GloVe embeddings, which you can read about [here](https://nlp.stanford.edu/projects/glove/). GloVe stands for \"Global Vectors for Word Representation\". It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics.\n",
    "\n",
    "Specifically, we will use the 200-dimensional GloVe embeddings of 400k words computed on a 2014 dump of English Wikipedia. You can download them [here](http://nlp.stanford.edu/data/glove.6B.zip).\n",
    "\n",
    "In addition, to check whether the pre-trained word embedding needs to be trained or not, we made the function below configurable for *trainable* parameter of Embedding object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "def load_pre_trained_embedding(word_index, max_word_length, trainable_for_embedding):\n",
    "    embeddings_index = {}\n",
    "    with open('./glove/glove.6B.200d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=max_word_length, weights=[embedding_matrix], trainable=trainable_for_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New (not-trained) word embedding\n",
    "New word embedding is created with no pre-trained weights, and it should be trainable always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_embedding(word_index, max_word_length, trainable_for_embedding):\n",
    "    # This is new embedding layer, so trainable must be True regardless the value of trainable_for_embedding\n",
    "    return Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=max_word_length, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Deep Learning Model\n",
    "\n",
    "To build a deep learning model, we basically use the sequential model of Keras.\n",
    "\n",
    "First, the Embedding layer is located. There are two options of setting embedding layers: using the pre-trained word embedding or training new embedding from scratch.\n",
    "\n",
    "Second, a series of **convolution 1D** and **pooling layers** are added according to typical CNN for text analysis. \n",
    "\n",
    "In order to check the effects of the number of convolution layers, we made the function below configurable to set the number of additional convolution layers.\n",
    "\n",
    "Then, after flattening layer, fully connected dense layers are added.\n",
    "Since this is a binary classification problem, we use the sigmoid function as an activation function for the final dense layer. If you try to predict a score of a review, it would be better to use 'softmax' function as the activation function.\n",
    "\n",
    "  - **Activation Function**  \n",
    "  The activation function is used as a decision making body at the output of a neuron. The neuron learns Linear or Non-linear decision boundaries based on the activation function.\n",
    "\n",
    "  It also has a normalizing effect on the neuron output which prevents the output of neurons after several layers to become very large, due to the cascading effect.  \n",
    "  \n",
    "  There are three most widely used activation functions:\n",
    "      - **Sigmoid**: It maps the input (x axis) to values between 0 and 1 (which may later results in the [vanishing gradient problem](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484).\n",
    "      - **Tanh**: It is similar to the sigmoid function butmaps the input to values between -1 and 1.\n",
    "      - **Rectified Linear Unit (ReLU)**: - It allows only positive values to pass through it. The negative values are mapped to zero.\n",
    "  - **Dropout**  \n",
    "  During training, when dropout is applied to a layer, some percentage of its neurons (a hyperparameter, with common values being between 20 and 50%) are randomly deactivated or “dropped out,” along with their connections. Which neurons are dropped out are constantly shuffled randomly during training. This forces the network to learn a more balanced representation, and **helps with overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(word_index, max_word_length, number_of_additional_conv_layers, use_pre_trained_embedding, trainable_for_embedding, number_of_filters, use_dropout, num_units):\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    embedding_func = new_embedding\n",
    "    if use_pre_trained_embedding: \n",
    "        embedding_func = load_pre_trained_embedding\n",
    "    model.add(embedding_func(word_index, max_word_length, trainable_for_embedding))\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    \n",
    "    for i in range(number_of_additional_conv_layers):\n",
    "        model.add(Conv1D(filters=number_of_filters, kernel_size=5, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=4))\n",
    "        if use_dropout:\n",
    "            model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=number_of_filters, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=10))\n",
    "    if use_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(units=num_units, activation='relu')) #num_units= [128,32]\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Deep Learning Models using Various Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous model accuracy\n",
    "\n",
    "[1] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n",
    "\n",
    "In the paper [1], the performance of machine learning modelsis in range of 67.42% to 88.89%. The model performed best in the cross validated Support Vector Machine (SVM) when concatenated with bag of words representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base model\n",
    "\n",
    "Now, all the functions defined before are combined in the function *build_and_train_model* as below.\n",
    "We compare the different models by changing the six parameters.\n",
    "Each model is trained by different combinations of six parameters.\n",
    "\n",
    "The meaning of the parameters are as follows:\n",
    "* **use_cleaned_docs**: Cleaning review documents or not. \n",
    "* **number_of_additional_conv_layers**: Number of additional convolution layers. Basically, one convolution layer is used (`number_of_additional_conv_layers`=0). If you want to add more convolution layers, we can adjust this parameter to a higher number. For experiment, we adjust the `number_of_additional_conv_layers`=2, so the total convolution layers becomes 3.\n",
    "* **use_pre_trained_embedding**: Using pre-trained embedding or not. If True, the GloVe embedding will be used as mentioned above.\n",
    "* **trainable_for_embedding**: Training the embedding layer with training data set or freezing. Note, when using the new embedding layer, then trainable_for_embedding should be True. \n",
    "* **number_of_filters**: Number of filters in the convolution layers (96 or 24).\n",
    "* **use_dropout**: Using Dropout or not. In the experiment, 50% percentage of its neurons are randomly deactivated.\n",
    "* **num_units**: Number of units in Dense layer. This reduces the capacity of network (128 or 32). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When setting up model configurations by combination of features, there are no cases of no embedding layers \n",
    "(`use_pre_trained_embedding`= **False** and `trainable_for_embedding` = **False**)\n",
    ", so these should be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "#modelpath = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "modelpath = \"./model/{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_acc', verbose=0, save_best_only=True)\n",
    "#early_stopping_callback = EarlyStopping(monitor='val_loss', mode='min', patience=2)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_acc', mode='max', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def build_and_train_model(use_cleaned_docs=False, \n",
    "                          number_of_additional_conv_layers=2, \n",
    "                          use_pre_trained_embedding=True,\n",
    "                          trainable_for_embedding=True,\n",
    "                          number_of_filters=96, \n",
    "                          use_dropout=True,\n",
    "                          num_units=128):\n",
    "    \n",
    "    train_positive_docs = global_train_positive_docs\n",
    "    train_negative_docs = global_train_negative_docs\n",
    "    test_positive_docs = global_test_positive_docs\n",
    "    test_negative_docs = global_test_negative_docs\n",
    "    \n",
    "    # clean up documents if required\n",
    "    if use_cleaned_docs:\n",
    "        train_positive_docs = cleaned_train_positive_docs\n",
    "        train_negative_docs = cleaned_train_negative_docs\n",
    "        test_positive_docs = cleaned_test_positive_docs\n",
    "        test_negative_docs = cleaned_test_negative_docs\n",
    "    \n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    train_docs = train_positive_docs + train_negative_docs\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    #\n",
    "    print('Fitted tokenizer on {} documents'.format(tokenizer.document_count))\n",
    "    #print('{} words in dictionary'.format(tokenizer.num_words))\n",
    "    print('Top 5 most common words are:', Counter(tokenizer.word_counts).most_common(5))\n",
    "    \n",
    "    # calculate maximum length of words in training docs\n",
    "    max_word_length = max([len(s.split()) for s in train_docs])\n",
    "    # get word_index\n",
    "    word_index = tokenizer.word_index\n",
    "    #print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    # encode data into two sequences: x = input, y = output\n",
    "    x_train, y_train = encode_data_set(tokenizer, train_positive_docs, train_negative_docs, max_word_length)\n",
    "    x_test, y_test = encode_data_set(tokenizer, test_positive_docs, test_negative_docs, max_word_length)\n",
    "\n",
    "    # build a model\n",
    "    model = build_model(word_index, max_word_length, number_of_additional_conv_layers, \n",
    "                        use_pre_trained_embedding, trainable_for_embedding, number_of_filters, use_dropout, num_units)\n",
    "    # fit network (Training)\n",
    "    history = model.fit(x_train, y_train, epochs=16, verbose=2, validation_data=(x_test, y_test), batch_size=96, callbacks=[TQDMNotebookCallback(), early_stopping_callback, checkpointer])\n",
    "    # evaluate\n",
    "    loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test Accuracy: %.2f%%' % (acc*100))\n",
    "    \n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "    del model\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start with the following configurations of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>use_cleaned_docs</th>\n",
       "      <th>number_of_additional_conv_layers</th>\n",
       "      <th>use_pre_trained_embedding</th>\n",
       "      <th>trainable_for_embedding</th>\n",
       "      <th>number_of_filters</th>\n",
       "      <th>use_dropout</th>\n",
       "      <th>num_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   use_cleaned_docs  number_of_additional_conv_layers  \\\n",
       "0             False                                 0   \n",
       "1             False                                 0   \n",
       "2             False                                 0   \n",
       "3             False                                 2   \n",
       "4             False                                 2   \n",
       "5             False                                 2   \n",
       "\n",
       "   use_pre_trained_embedding  trainable_for_embedding  number_of_filters  \\\n",
       "0                       True                    False                 96   \n",
       "1                      False                     True                 96   \n",
       "2                       True                     True                 96   \n",
       "3                       True                    False                 96   \n",
       "4                      False                     True                 96   \n",
       "5                       True                     True                 96   \n",
       "\n",
       "   use_dropout  num_units  \n",
       "0        False        128  \n",
       "1        False        128  \n",
       "2        False        128  \n",
       "3        False        128  \n",
       "4        False        128  \n",
       "5        False        128  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_conditions = []\n",
    "\n",
    "# for use_cleaned_docs in [False]:\n",
    "#     for number_of_additional_conv_layers in [2]: #[2, 0]\n",
    "#         for use_pre_trained_embedding in [True, False]:\n",
    "#             for trainable_for_embedding in [True, False]:\n",
    "#                 if (not use_pre_trained_embedding) and (not trainable_for_embedding):\n",
    "#                     continue\n",
    "#                 for number_of_filters in [96]: #[96, 24]\n",
    "#                     for use_dropout in [False]: #[True, False]\n",
    "#                         exp_conditions.append({\n",
    "#                             'use_cleaned_docs': use_cleaned_docs, \n",
    "#                             'number_of_additional_conv_layers': number_of_additional_conv_layers,\n",
    "#                             'use_pre_trained_embedding': use_pre_trained_embedding,\n",
    "#                             'trainable_for_embedding': trainable_for_embedding,\n",
    "#                             'number_of_filters': number_of_filters,\n",
    "#                             'use_dropout': use_dropout,\n",
    "#                             'num_units' : 128\n",
    "#                         })\n",
    "     \n",
    "\n",
    "# for use_pre_trained_embedding in [True, False]:\n",
    "#     for trainable_for_embedding in [True, False]:\n",
    "#         if (not use_pre_trained_embedding) and (not trainable_for_embedding):\n",
    "#             continue\n",
    "#         exp_conditions.append({\n",
    "#             'use_cleaned_docs': False, \n",
    "#             'number_of_additional_conv_layers': 2,\n",
    "#             'use_pre_trained_embedding': use_pre_trained_embedding,\n",
    "#             'trainable_for_embedding': trainable_for_embedding,\n",
    "#             'number_of_filters': 96,\n",
    "#             'use_dropout': False,\n",
    "#             'num_units' : 128\n",
    "#         })\n",
    "\n",
    "exp_conditions.append({\n",
    "        'use_cleaned_docs': False, \n",
    "        'number_of_additional_conv_layers': 0,\n",
    "        'use_pre_trained_embedding': True,\n",
    "        'trainable_for_embedding': False,\n",
    "        'number_of_filters': 96,\n",
    "        'use_dropout': False,\n",
    "        'num_units' : 128\n",
    "})\n",
    "\n",
    "exp_conditions.append({\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 0,\n",
    "    'use_pre_trained_embedding': False,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': False,\n",
    "    'num_units' : 128\n",
    "})\n",
    "    \n",
    "exp_conditions.append({\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 0,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': False,\n",
    "    'num_units' : 128\n",
    "})\n",
    "    \n",
    "exp_conditions.append({\n",
    "        'use_cleaned_docs': False, \n",
    "        'number_of_additional_conv_layers': 2,\n",
    "        'use_pre_trained_embedding': True,\n",
    "        'trainable_for_embedding': False,\n",
    "        'number_of_filters': 96,\n",
    "        'use_dropout': False,\n",
    "        'num_units' : 128\n",
    "})\n",
    "\n",
    "exp_conditions.append({\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': False,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': False,\n",
    "    'num_units' : 128\n",
    "})\n",
    "    \n",
    "exp_conditions.append({\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': False,\n",
    "    'num_units' : 128\n",
    "})\n",
    "\n",
    "columns_display = ['use_cleaned_docs','number_of_additional_conv_layers', 'use_pre_trained_embedding', 'trainable_for_embedding', 'number_of_filters', 'use_dropout', 'num_units']\n",
    "exp_cond_df = pd.DataFrame(exp_conditions, columns=columns_display)\n",
    "exp_cond_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build and train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>use_cleaned_docs</th>\n",
       "      <th>number_of_additional_conv_layers</th>\n",
       "      <th>use_pre_trained_embedding</th>\n",
       "      <th>trainable_for_embedding</th>\n",
       "      <th>number_of_filters</th>\n",
       "      <th>use_dropout</th>\n",
       "      <th>num_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   use_cleaned_docs  number_of_additional_conv_layers  \\\n",
       "0             False                                 0   \n",
       "\n",
       "   use_pre_trained_embedding  trainable_for_embedding  number_of_filters  \\\n",
       "0                       True                    False                 96   \n",
       "\n",
       "   use_dropout  num_units  \n",
       "0        False        128  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted tokenizer on 25000 documents\n",
      "Top 5 most common words are: [('the', 336148), ('and', 164097), ('a', 163040), ('of', 145847), ('to', 135708)]\n",
      "WARNING:tensorflow:From /home/igsong/.local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[{{node _arg_embedding_1/Placeholder_0_0}}]]\n\t [[node embedding_1/Assign (defined at /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2465) ]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[{{node _arg_embedding_1/Placeholder_0_0}}]]\n\t [[{{node embedding_1/Assign}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0e842581973d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_cond_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_and_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mexp_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mexp_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-033911b9db54>\u001b[0m in \u001b[0;36mbuild_and_train_model\u001b[0;34m(use_cleaned_docs, number_of_additional_conv_layers, use_pre_trained_embedding, trainable_for_embedding, number_of_filters, use_dropout, num_units)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# build a model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     model = build_model(word_index, max_word_length, number_of_additional_conv_layers, \n\u001b[0;32m---> 44\u001b[0;31m                         use_pre_trained_embedding, trainable_for_embedding, number_of_filters, use_dropout, num_units)\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;31m# fit network (Training)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTQDMNotebookCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b5e1c52b7e2e>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(word_index, max_word_length, number_of_additional_conv_layers, use_pre_trained_embedding, trainable_for_embedding, number_of_filters, use_dropout, num_units)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_pre_trained_embedding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0membedding_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pre_trained_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_word_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_for_embedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_dropout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                     \u001b[0mset_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;31m# Load weights that were specified at layer instantiation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1057\u001b[0m                                  'provided weight shape ' + str(w.shape))\n\u001b[1;32m   1058\u001b[0m             \u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1059\u001b[0;31m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2468\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2470\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[{{node _arg_embedding_1/Placeholder_0_0}}]]\n\t [[node embedding_1/Assign (defined at /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2465) ]]"
     ]
    }
   ],
   "source": [
    "exp_result = []\n",
    "\n",
    "for i, exp_cond in enumerate(exp_conditions):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    K.set_session(sess)\n",
    "    display(exp_cond_df[i:i+1])\n",
    "    history = build_and_train_model(**exp_cond)\n",
    "    print(history)\n",
    "    exp_result.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "columns_display = ['use_cleaned_docs','number_of_additional_conv_layers', 'use_pre_trained_embedding', 'trainable_for_embedding', 'number_of_filters', 'use_dropout', 'num_units', 'val_acc']\n",
    "                   \n",
    "def drawModelAcc_Loss(history, exp_cond):\n",
    "    exp_cond['val_acc'] = max(history.history['val_acc'])\n",
    "    #exp_cond['val_acc'] = history.history['val_acc'][-1]\n",
    "    #exp_cond['num'] = i\n",
    "    exp_cond_row_df = pd.DataFrame([exp_cond], columns=columns_display)\n",
    "    #print(history.history['val_acc'])\n",
    "    display(exp_cond_row_df)\n",
    "    #display(exp_cond_row_df.to_string(index=False))\n",
    "    #i = i + 1\n",
    "    #print(exp_cond)\n",
    "    \n",
    "    plt.figure()\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "\n",
    "    plt.title('Model accuracy and loss')\n",
    "    plt.ylabel('accuracy and loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_acc', 'test_acc', 'train_loss', 'test_loss'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "#     plt.figure()\n",
    "#     plt.plot(history.history['loss'])\n",
    "#     plt.plot(history.history['val_loss'])\n",
    "#     plt.title('Model loss')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()\n",
    "\n",
    "#i = 0\n",
    "for history, exp_cond in zip(exp_result, exp_conditions):\n",
    "    #exp_cond['val_acc'] = history.history['val_acc'][-1]\n",
    "    drawModelAcc_Loss(history, exp_cond)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond_df = pd.DataFrame(exp_conditions, columns=columns_display)\n",
    "exp_cond_df['val_acc'] = exp_cond_df['val_acc'].apply(lambda x:x*100)\n",
    "exp_cond_df['val_acc'] = exp_cond_df['val_acc'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_cond_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_build_and_train_model(exp_cond):\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    K.set_session(sess)\n",
    "    history = build_and_train_model(exp_cond)\n",
    "    #print(history)\n",
    "    exp_result.append(history)\n",
    "    drawModelAcc_Loss(history, exp_cond)\n",
    "    \n",
    "    #one dict to Dataframe\n",
    "    df_exp_cond = pd.DataFrame([exp_cond], columns=columns_display)\n",
    "    df_exp_cond['val_acc'] = df_exp_cond['val_acc'].apply(lambda x:x*100)\n",
    "    df_exp_cond['val_acc'] = df_exp_cond['val_acc'].round(2)\n",
    "    return df_exp_cond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preventing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the callbacks functions of `EarlyStopping` and `ModelCheckpoint`.\n",
    "\n",
    "One way to avoid overfitting is to terminate the process early.\n",
    "We used the `EarlyStopping` function and set the arguments `monitor`= val_acc (test accuracy) and `patience`=2.\n",
    "The `patience` indicates the number of epochs with no improvement after which training will be stopped.\n",
    "\n",
    "The `ModelCheckpoint`callback saves the model after every epoch. \n",
    "\n",
    "\n",
    "Here the most common ways to prevent overfitting in neural networks:\n",
    "\n",
    "- Get more training data.\n",
    "- Reduce the capacity of the network.\n",
    "- Add weight regularization.\n",
    "- Add dropout.\n",
    "- Data-augmentation\n",
    "- Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'number_of_additional_conv_layers' = 0 && network size reduction && dropout true or false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 0,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 24,\n",
    "    'use_dropout': False,\n",
    "    'num_units': 32\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 0,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 24,\n",
    "    'use_dropout': True,\n",
    "    'num_units': 32\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce the network size 1. 'number_of_filters' : 96 to 48 and 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 48,\n",
    "    'use_dropout': False,\n",
    "    'num_units': 128\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 24,\n",
    "    'use_dropout': False,\n",
    "    'num_units': 128\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce the network size 2-1. 'num_units': 128 to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': False,\n",
    "    'num_units': 32\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce the network size 2-2. 'num_units': 128 to 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': False,\n",
    "    'num_units': 64\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce the network size 2-3. 'num_units': 128 to 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': False,\n",
    "    'num_units': 8\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce the network size 3. 'number_of_filters' : 96 to 24 and 'num_units': 128 to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 24,\n",
    "    'use_dropout': False,\n",
    "    'num_units': 32\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': True,\n",
    "    'num_units': 128\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 48,\n",
    "    'use_dropout': True,\n",
    "    'num_units': 128\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use_clean_docs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use_clean_docs = True\n",
    "exp_cond = {\n",
    "    'use_cleaned_docs': True, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': True,\n",
    "    'num_units': 128\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#best\n",
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': False,\n",
    "    'num_units': 128\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best\n",
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 96,\n",
    "    'use_dropout': True,\n",
    "    'num_units': 128\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best\n",
    "exp_cond = {\n",
    "    'use_cleaned_docs': False, \n",
    "    'number_of_additional_conv_layers': 2,\n",
    "    'use_pre_trained_embedding': True,\n",
    "    'trainable_for_embedding': True,\n",
    "    'number_of_filters': 24,\n",
    "    'use_dropout': True,\n",
    "    'num_units': 128\n",
    "}\n",
    "\n",
    "df_exp_cond_result = draw_build_and_train_model(exp_cond)\n",
    "exp_cond_df = exp_cond_df.append(df_exp_cond_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#final\n",
    "exp_cond_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond_df = exp_cond_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(\"max accuracy\\nindex: {}, accuracy: {}\".format(exp_cond_df['val_acc'].idxmax(), exp_cond_df['val_acc'].max()))\n",
    "# exp_cond_df.iloc[exp_cond_df['val_acc'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exp_cond_df['val_acc'].nlargest(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exp_cond_df.sort_values(by=['val_acc'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Overall accuracy**  \n",
    "The accuracy is best as 90.18% in the following condiiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond_df.iloc[[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Word embedding**  \n",
    "For the word embedding, the **use of sole pre-trained word embedding produces the least performance**. This can be interpreted that Embedding model have limitations to be generalized and should be differentiated into their contexts. Thus, **the use of the new training word embedding was the better choice.** Finally, **the mixed use of pre-trained word embedding and new training word embedding produces the best performance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond_df.iloc[[0,1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Network size: Number of filter in convolutional neural network and Number of units in Dense layer**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The larger number of filter (96,48,24) in convolutional neural network gives better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond_df.iloc[[0,3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The larger number of units the Dense layer (128, 64, 32, 8) has, the better performance, but not necessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond_df.iloc[[0,4,5,6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- But, when reducing the number of filter in convolutional neural network and number of units in Dense layer together, it returns better performance than reducing one for each, even it still has lower accuracy compared to the model having higher numbers in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(exp_cond_df.iloc[[3,4,5,6,7]])\n",
    "display(exp_cond_df.iloc[[0,7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Dropout**  \n",
    "When the size of the network is large, the dropout helps avoid overfitting. However, when the network size is small, it is not effective since too much data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond_df.iloc[[0,8,9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Cleaning documents**  \n",
    "In deep learning modeling, it seems that the pre-processing (cleaning) does not seem to affect the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond_df.iloc[[0,10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Nubmer of convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_cond_df.iloc[[0,11,12]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
